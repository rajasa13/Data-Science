{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('/Data/Visual Studio Code/Data Science/Dataset/UNSW-NB15/UNSW_NB15_training-set.csv')\n",
    "data_test = pd.read_csv('/Data/Visual Studio Code/Data Science/Dataset/UNSW-NB15/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size: \" + str(data_train.size))\n",
    "print(\"Shape: \" + str(data_train.shape))\n",
    "print(\"-----------------------\")\n",
    "print(data_train.value_counts('attack_cat'))\n",
    "print(\"-----------------------\")\n",
    "print(data_train.value_counts('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing value and duplicate if any\n",
    "data_train.dropna(inplace=True)\n",
    "data_train.drop_duplicates(inplace=True)\n",
    "\n",
    "data_test.dropna(inplace=True)\n",
    "data_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack and Normal distribution\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.countplot(data=data_train, x=\"label\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.title(\"Label Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack Category\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.countplot(data=data_train, x=\"attack_cat\")\n",
    "plt.xlabel(\"Attack Category\")\n",
    "plt.title(\"Attack Category Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All_feature\n",
    "all_feature = data_train.columns\n",
    "all_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Attack and Normal from dataframe\n",
    "data_train_attack = data_train[data_train['label'] == 1]\n",
    "data_test_attack = data_test[data_test['label'] == 1]\n",
    "\n",
    "#Optional\n",
    "#data_train_normal = data_train[data_train['label'] == 0]\n",
    "#data_test_normal = data_test[data_test['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack Category Distribution\n",
    "plt.figure(figsize=(12, 12))\n",
    "data_train_attack['attack_cat'].value_counts().plot(kind='pie', autopct='%1.2f%%')\n",
    "plt.title(\"Attack Catagory Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack Category\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.countplot(data=data_train_attack, x=\"attack_cat\")\n",
    "plt.xlabel(\"Attack Category\")\n",
    "plt.title(\"Attack Category Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode attack_cat\n",
    "le = LabelEncoder()\n",
    "data_train_attack['attack_cat'] = le.fit_transform(data_train_attack['attack_cat'])\n",
    "\n",
    "#Encode state\n",
    "data_train_attack['state'] = le.fit_transform(data_train_attack['state'])\n",
    "\n",
    "#Encode service\n",
    "data_train_attack['service'] = le.fit_transform(data_train_attack['service'])\n",
    "\n",
    "#Encode proto\n",
    "data_train_attack['proto'] = le.fit_transform(data_train_attack['proto'])\n",
    "\n",
    "data_train_attack.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode attack_cat\n",
    "data_test_attack['attack_cat'] = le.fit_transform(data_test_attack['attack_cat'])\n",
    "\n",
    "#Encode state\n",
    "data_test_attack['state'] = le.fit_transform(data_test_attack['state'])\n",
    "\n",
    "#Encode service\n",
    "data_test_attack['service'] = le.fit_transform(data_test_attack['service'])\n",
    "\n",
    "#Encode proto\n",
    "data_test_attack['proto'] = le.fit_transform(data_test_attack['proto'])\n",
    "\n",
    "data_test_attack.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Attack Category Encoding\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.countplot(data=data_train_attack, x=\"attack_cat\")\n",
    "plt.xlabel(\"Attack Category\")\n",
    "plt.title(\"Attack Category Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_norm = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', \n",
    "                'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', \n",
    "                'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', \n",
    "                'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', \n",
    "                'ct_srv_dst', 'is_sm_ips_ports']\n",
    "\n",
    "data_train_attack[cols_to_norm] = scaler.fit_transform(data_train_attack[cols_to_norm])\n",
    "data_test_attack[cols_to_norm] = scaler.fit_transform(data_test_attack[cols_to_norm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between feature\n",
    "#%matplotlib inline\n",
    "plt.figure(figsize=[32,32])\n",
    "sns.heatmap(data_train_attack.corr(), annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "feature = data_train_attack.drop(['id', 'dinpkt', 'ct_ftp_cmd', 'ct_src_dport_ltm', 'is_sm_ips_ports', 'label'], axis=1)\n",
    "feature.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between feature\n",
    "#%matplotlib inline\n",
    "plt.figure(figsize=[32,32])\n",
    "sns.heatmap(feature.corr(), annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into dependent and independent feature\n",
    "X_train = feature.drop(columns=['attack_cat'])\n",
    "y_train = feature[\"attack_cat\"]\n",
    "\n",
    "X_test = data_test_attack.drop(columns=['attack_cat', 'id', 'dinpkt', 'ct_ftp_cmd', 'ct_src_dport_ltm', 'is_sm_ips_ports', 'label'])\n",
    "y_test = data_test_attack[\"attack_cat\"]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE-Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl = SMOTETomek(sampling_strategy='auto')\n",
    "#stl = SMOTETomek(sampling_strategy=dict({2: 8000, 3: 7800}))\n",
    "X_stl, y_stl = stl.fit_resample(X_train, y_train)\n",
    "\n",
    "X_stl.shape, y_stl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe from X_stl and y_stl to count every attack\n",
    "data_stl = pd.concat([X_stl, y_stl], axis=1)\n",
    "\n",
    "sns.countplot(data=data_stl, x=\"attack_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_stl.drop_duplicates(inplace=True)\n",
    "print(data_stl.value_counts('attack_cat'))\n",
    "print(\"--------------------\")\n",
    "print(\"Shape: \" + str(data_stl.shape))\n",
    "print(\"--------------------\")\n",
    "print(\"Duplicates: \" + str(data_stl.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holder to store model performance\n",
    "from sklearn import metrics\n",
    "ML_Model = []\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "\n",
    "#function to storing the results\n",
    "def storeResults(model, a, b, c, d):\n",
    "    ML_Model.append(model)\n",
    "    accuracy.append(round(a, 5))\n",
    "    f1_score.append(round(b, 5))\n",
    "    recall.append(round(c, 5))\n",
    "    precision.append(round(d, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to computing performance and computing performance using classification report, then stroing the results\n",
    "def model_report(modelName, y_train, y_test, p_train, p_test):\n",
    "    print(\"Model:{}\\n\".format(modelName))\n",
    "\n",
    "    #computing accuracy score\n",
    "    acc_train = metrics.accuracy_score(y_train, p_train)\n",
    "    acc_test = metrics.accuracy_score(y_test, p_test)\n",
    "    print(\"Accuracy on training Data: {:.5f}\".format(acc_train))\n",
    "    print(\"Accuracy on test Data: {:.5f}\\n\".format(acc_test))\n",
    "\n",
    "    #computing f1 score\n",
    "    f1_score_train = metrics.f1_score(y_train, p_train, average='macro')\n",
    "    f1_score_test = metrics.f1_score(y_test, p_test, average='macro')\n",
    "    print(\"F1 score on training Data: {:.5f}\".format(f1_score_train))\n",
    "    print(\"F1 score on test Data: {:.5f}\\n\".format(f1_score_test))\n",
    "\n",
    "    #computing recall score\n",
    "    recall_score_train = metrics.recall_score(y_train, p_train, average='macro')\n",
    "    recall_score_test = metrics.recall_score(y_test, p_test, average='macro')\n",
    "    print(\"Recall score on training Data: {:.5f}\".format(recall_score_train))\n",
    "    print(\"Recall score on test Data: {:.5f}\\n\".format(recall_score_test))\n",
    "\n",
    "    #computing precision score\n",
    "    precision_score_train = metrics.precision_score(y_train, p_train, average='macro')\n",
    "    precision_score_test = metrics.precision_score(y_test, p_test, average='macro')\n",
    "    print(\"Precision on training Data: {:.5f}\".format(precision_score_train))\n",
    "    print(\"Precision on test Data: {:.5f}\\n\".format(precision_score_test))\n",
    "\n",
    "    #computing classification report of model\n",
    "    print(\"Classification Report\")\n",
    "    print(metrics.classification_report(y_test, p_test))\n",
    "\n",
    "    #storing results\n",
    "    storeResults(modelName, acc_test, f1_score_test, recall_score_test, precision_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Testing, Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#instantiate the model\n",
    "destree = DecisionTreeClassifier(max_depth=11)\n",
    "\n",
    "#fit the model\n",
    "destree.fit(X_stl, y_stl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "p_train_destree = destree.predict(X_stl)\n",
    "p_test_destree = destree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call report function\n",
    "model_report(str(destree), y_stl, y_test, p_train_destree, p_test_destree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the train and test model accuracy\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#try max_depth from 1 to 30\n",
    "depth = range(1, 30)\n",
    "for n in depth:\n",
    "    destree_test = DecisionTreeClassifier(max_depth=n)\n",
    "\n",
    "    destree_test.fit(X_stl, y_stl)\n",
    "    #record training set accuracy\n",
    "    training_accuracy.append(destree_test.score(X_stl, y_stl))\n",
    "    #recor generalization accuracy\n",
    "    test_accuracy.append(destree_test.score(X_test, y_test))\n",
    "\n",
    "#plotting the training & testing accuracy for max_depth from 1 to 30\n",
    "plt.plot(depth, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(depth, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#instantiate the model\n",
    "ranfor = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "#fit the model\n",
    "ranfor.fit(X_stl, y_stl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "p_train_ranfor = ranfor.predict(X_stl)\n",
    "p_test_ranfor = ranfor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call report function\n",
    "model_report(str(ranfor), y_stl, y_test, p_train_ranfor, p_test_ranfor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the train and test model accuracy\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#try n_estimators from 1 to 20\n",
    "n_est = range(1, 20)\n",
    "for n in n_est:\n",
    "    ranfor_test = RandomForestClassifier(n_estimators=n)\n",
    "\n",
    "    ranfor_test.fit(X_stl, y_stl)\n",
    "    #record training set accuracy\n",
    "    training_accuracy.append(ranfor_test.score(X_stl, y_stl))\n",
    "    #recor generalization accuracy\n",
    "    test_accuracy.append(ranfor_test.score(X_test, y_test))\n",
    "\n",
    "#plotting the training & testing accuracy for n_estimators from 1 to 20\n",
    "plt.plot(n_est, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(n_est, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gradient boosting classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#instantiate the model\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "#fit the model\n",
    "gbc.fit(X_stl, y_stl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "p_train_gbc = gbc.predict(X_stl)\n",
    "p_test_gbc = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call report function\n",
    "model_report(str(gbc), y_stl, y_test, p_train_gbc, p_test_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#instantiate the model\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "#fit the model\n",
    "xgb.fit(X_stl, y_stl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "p_train_xgb = xgb.predict(X_stl)\n",
    "p_test_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call report function\n",
    "model_report(str(xgb), y_stl, y_test, p_train_xgb, p_test_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "result = pd.DataFrame({'ML Model' : ML_Model,\n",
    "                       'Accuracy' : accuracy,\n",
    "                       'F1 Score' : f1_score,\n",
    "                       'Recall' : recall,\n",
    "                       'Precision' : precision\n",
    "                       })\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the dataframe on accuracy\n",
    "sorted_result = result.sort_values(by=['Recall'], ascending=False).reset_index(drop=True)\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save result to csv file\n",
    "sorted_result.to_csv('encode_smotetomek_unswnb15_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate recall from confusion matrix\n",
    "def calculate_recall(confusion_matrix):\n",
    "    num_class = len(confusion_matrix)\n",
    "    recalls = []\n",
    "\n",
    "    for i in range(num_class):\n",
    "        true_positive = confusion_matrix[i][i]\n",
    "        false_negative = sum(confusion_matrix[i]) - true_positive\n",
    "\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display the recall score\n",
    "def display_recall(recalls):\n",
    "    print('Recall Scores for each class:')\n",
    "    print('Analysis:', recalls[0])\n",
    "    print('Backdoor:', recalls[1])\n",
    "    print('DoS:', recalls[2])\n",
    "    print('Exploits:', recalls[3])\n",
    "    print('Fuzzers:', recalls[4])\n",
    "    print('Generic:', recalls[5])\n",
    "    print('Reconnaissance:', recalls[6])\n",
    "    print('Shellcode:', recalls[7])\n",
    "    print('Worms:', recalls[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(y_test, p_test_destree)\n",
    "matrix_df = pd.DataFrame(matrix, \n",
    "                         index= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'], \n",
    "                         columns= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(matrix_df, fmt='.0f', annot=True)\n",
    "plt.title('Confusion Matrix (Decision Tree as Classifier)')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = calculate_recall(matrix)\n",
    "display_recall(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(y_test, p_test_ranfor)\n",
    "matrix_df = pd.DataFrame(matrix, \n",
    "                         index= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'], \n",
    "                         columns= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(matrix_df, fmt='.0f', annot=True)\n",
    "plt.title('Confusion Matrix (Random Forest as Classifier)')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = calculate_recall(matrix)\n",
    "display_recall(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(y_test, p_test_gbc)\n",
    "matrix_df = pd.DataFrame(matrix, \n",
    "                         index= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'], \n",
    "                         columns= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(matrix_df, fmt='.0f', annot=True)\n",
    "plt.title('Confusion Matrix (Gradient Boosting as Classifier)')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = calculate_recall(matrix)\n",
    "display_recall(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(y_test, p_test_xgb)\n",
    "matrix_df = pd.DataFrame(matrix, \n",
    "                         index= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'], \n",
    "                         columns= ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(matrix_df, fmt='.0f', annot=True)\n",
    "plt.title('Confusion Matrix (XGBoost as Classifier)')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = calculate_recall(matrix)\n",
    "display_recall(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the decision tree trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/destree.pkl', 'wb') as model_file:\n",
    "    pickle.dump(destree, model_file)\n",
    "\n",
    "# save the random forest trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/ranfor.pkl', 'wb') as model_file:\n",
    "    pickle.dump(ranfor, model_file)\n",
    "\n",
    "# save the gradient boosting trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/gbc.pkl', 'wb') as model_file:\n",
    "    pickle.dump(gbc, model_file)\n",
    "\n",
    "# save the xgboost trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/xgb.pkl', 'wb') as model_file:\n",
    "    pickle.dump(xgb, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the decision tree trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/destree.pkl', 'rb') as model_file:\n",
    "    destree = pickle.load(model_file)\n",
    "\n",
    "# load the random forest trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/ranfor.pkl', 'rb') as model_file:\n",
    "    ranfor = pickle.load(model_file)\n",
    "\n",
    "# load the gradient boosting trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/gbc.pkl', 'rb') as model_file:\n",
    "    gbc = pickle.load(model_file)\n",
    "\n",
    "# load the xgboost trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/xgb.pkl', 'rb') as model_file:\n",
    "    xgb = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the decision tree trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_destree.pkl', 'wb') as model_file:\n",
    "    pickle.dump(p_test_destree, model_file)\n",
    "\n",
    "# save the random forest trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_ranfor.pkl', 'wb') as model_file:\n",
    "    pickle.dump(p_test_ranfor, model_file)\n",
    "\n",
    "# save the gradient boosting trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_gbc.pkl', 'wb') as model_file:\n",
    "    pickle.dump(p_test_gbc, model_file)\n",
    "\n",
    "# save the xgboost trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_xgb.pkl', 'wb') as model_file:\n",
    "    pickle.dump(p_test_xgb, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the decision tree trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_destree.pkl', 'rb') as model_file:\n",
    "    p_test_destree = pickle.load(model_file)\n",
    "\n",
    "# load the random forest trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_ranfor.pkl', 'rb') as model_file:\n",
    "    p_test_ranfor = pickle.load(model_file)\n",
    "\n",
    "# load the gradient boosting trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_gbc.pkl', 'rb') as model_file:\n",
    "    p_test_gbc = pickle.load(model_file)\n",
    "\n",
    "# load the xgboost trained model\n",
    "with open('/Data/Visual Studio Code/Data Science/Notebook/UNSW-NB15/Balanced/Normalize/SMOTETOMEK/Saved/p_test_xgb.pkl', 'rb') as model_file:\n",
    "    p_test_xgb = pickle.load(model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
